{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习框架 PyTorch Lightning 和医学图像分析框架 MONAI 教程\n",
    "\n",
    "[Where to find this tutorial?](https://github.com/tobiasforest/notes)\n",
    "\n",
    "- **作者**：林日中 `rizhonglin[at]tongji[dot]edu[dot]cn`\n",
    "- **日期**：2023 年 11 月 15 日\n",
    "- **地点**：上海（腾讯会议）\n",
    "\n",
    "_The images in this tutorial are sourced from the official websites of [Lightning](https://lightning.ai) and [MONAI](https://monai.io). We hereby acknowledge and appreciate the creators for their excellent work. These images are utilized solely for educational purposes, with all rights recognized as belonging to the original authors. Their contributions to education and academic research are highly valued. This tutorial is intended strictly for non-commercial use, serving as a resource for study and research._\n",
    "\n",
    "_本教程中的图片来自 [Lightning](https://lightning.ai) 和 [MONAI](https://monai.io) 的官方网站。我们在此对创作者的出色工作表示感谢。这些图片仅用于教育目的，所有权利归原作者所有。我们高度评价他们为教育和学术研究做出的贡献。本教程仅供非商业使用，可作为学习和研究的资源。_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lightning AI\n",
    "\n",
    "<img alt=\"Lightning\" src=\"images/PyTorch-to-Fabric-Spectrum.png\" width=\"2500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### <img alt=\"Lightning Fabric\" src=\"images/Lightning-Fabric.png\" width=\"500\">\n",
    "\n",
    "Lightning Fabric 是一种快速且轻量的方法，用于扩展 PyTorch 模型，无需编写大量代码。您可以在 5 行代码内将 PyTorch 代码转换为 Lightning Fabric，从而获得对最先进的分布式训练功能（如 DDP、FSDP、DeepSpeed、混合精度等）的访问，以扩展最大的十亿参数模型。\n",
    "\n",
    "Fabric 与 Lightning 的完全不同的几个关键方面包括：快速实现（无需重构代码），最大化灵活性（编写自己的训练和/或推理逻辑），以及最大化控制（一切都是可选的，可根据需要逐步添加更多功能）。\n",
    "\n",
    "### <img alt=\"PyTorch Lightning\" src=\"images/Pytorch-Lightning.png\" width=\"500\">\n",
    "\n",
    "PyTorch Lightning 是一个轻量级的基于 PyTorch 的高层次模型接口，它提供了一种简单的方式来组织 PyTorch 代码，使得代码更加模块化、可读性更强、可维护性更高。它的设计目标是让研究人员专注于模型的设计，而不是训练过程的实现。\n",
    "\n",
    "它的核心思想是将训练过程分为 5 个部分：`LightningModule`、`LightningDataModule`、`Trainer`、`Callbacks` 和 `LightningLogger`。\n",
    "\n",
    "- `LightningModule` 是模型的核心，它包含了模型的定义、前向传播、损失函数、优化器等；\n",
    "- `LightningDataModule` 是数据的核心，它包含了数据的加载、预处理、划分等；\n",
    "- `Trainer` 是训练过程的核心，它包含了训练过程的超参数、优化器、学习率调整策略、训练过程的配置等；\n",
    "- `Callbacks` 是训练过程的钩子函数，它包含了训练过程中的一些回调函数，如模型保存、学习率调整、训练过程可视化等；\n",
    "- `LightningLogger` 是训练过程的日志记录器，它包含了训练过程中的一些日志记录，如训练过程的可视化、训练过程的日志记录等。\n",
    "\n",
    "最新的 PyTorch Lightning 教程包括如何在 Cifar10 上训练 Resnet 以达到 94% 的准确率，如何使用 DataModules，以及如何使用 Fine-Tuning Scheduler 来微调模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <img alt=\"MONAI\" src=\"images/MONAI.png\" width=\"500\">\n",
    "\n",
    "MONAI (Medical Open Network for AI) 是一个开源的医学图像分析框架，它旨在提供一个用于医学图像分析的端到端的深度学习框架，以加速 AI 在医学图像分析领域的研究和应用。\n",
    "\n",
    "MONAI 提供了一系列的工具，如数据加载、数据预处理、数据划分、模型定义、模型训练、模型评估、模型推理等，它们都是基于 PyTorch 实现的，因此可以很方便地与 PyTorch 结合使用。MONAI 教程可以在其 GitHub 页面上找到，提供了丰富的资源和示例，以帮助用户快速上手和应用 MONAI。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning 的安装\n",
    "\n",
    "Lightning 的安装非常简单。\n",
    "\n",
    "若想用 pip 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "pip install lightning\n",
    "```\n",
    "\n",
    "若想用 conda 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "conda install lightning -c conda-forge\n",
    "```\n",
    "\n",
    "安装完成后，可以使用以下命令查看版本信息：\n",
    "\n",
    "```bash\n",
    "lightning --version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import pytorch_lightning,lightning_fabric\" || pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 代码实现 MNIST 分类\n",
    "\n",
    "下面我们以训练一个简单的 MNIST 分类模型为例，来介绍如何从 PyTorch 迁移到 Fabric 和 Lightning。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T10:11:11.580590249Z",
     "start_time": "2023-06-18T10:10:04.424366012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 24571221.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/train-images-idx3-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 2292630.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/train-labels-idx1-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 15049301.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 21429166.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/rizhong/Documents/r/100400_数据挖掘/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 750/750 [00:06<00:00, 108.13batch/s, Train Loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4993, Validation Accuracy: 85.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 750/750 [00:06<00:00, 113.25batch/s, Train Loss=0.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3833, Validation Accuracy: 89.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 750/750 [00:06<00:00, 111.68batch/s, Train Loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3338, Validation Accuracy: 89.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 750/750 [00:06<00:00, 113.25batch/s, Train Loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3146, Validation Accuracy: 90.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 750/750 [00:06<00:00, 120.86batch/s, Train Loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2940, Validation Accuracy: 91.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 750/750 [00:06<00:00, 119.67batch/s, Train Loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2674, Validation Accuracy: 92.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 750/750 [00:06<00:00, 113.38batch/s, Train Loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2500, Validation Accuracy: 92.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 750/750 [00:06<00:00, 112.65batch/s, Train Loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2443, Validation Accuracy: 92.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 750/750 [00:06<00:00, 113.29batch/s, Train Loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2189, Validation Accuracy: 93.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 750/750 [00:06<00:00, 111.64batch/s, Train Loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2126, Validation Accuracy: 93.79%\n",
      "Test Accuracy: 94.09%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "validation_split = 0.2\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset = datasets.MNIST(root=os.getcwd(), train=True, transform=transform, download=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = datasets.MNIST(root=os.getcwd(), train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = Net().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"Train Loss\": train_loss / ((pbar.n - 1) * train_loader.batch_size + images.size(0))})\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Print validation metrics\n",
    "    tqdm.write(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print the accuracy on the test set\n",
    "accuracy = 100 * total_correct / total_samples\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), \"mnist.ckpt\")\n",
    "\n",
    "# Load the model checkpoint\n",
    "model.load_state_dict(torch.load(\"mnist.ckpt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这份 PyTorch 代码实现了一个用于手写数字识别的神经网络模型。代码主要包括以下几个部分：\n",
    "\n",
    "1. **导入所需的库**：包括 os、torch 以及 torch 的相关模块、数据集模块以及进度条模块。\n",
    "2. **设置设备**：根据可用的硬件情况，将模型的计算设备设置为 CPU 或 GPU。\n",
    "3. **定义神经网络模型的架构**：定义了一个包含三个全连接层的模型，输入大小为 784，输出大小为 10。\n",
    "4. **定义训练参数**：包括批处理大小、学习率、训练周期数和验证集的比例。\n",
    "5. **加载 MNIST 数据集**：使用 torchvision 中的 datasets 模块加载 MNIST 数据集，并进行数据预处理。\n",
    "6. **将数据集划分为训练集和验证集**：使用 random_split 函数将数据集划分为训练集和验证集。\n",
    "7. **创建数据加载器**：使用 DataLoader 模块创建训练集、验证集和测试集的数据加载器，用于批量加载数据。\n",
    "8. **初始化模型并将其移动到设备上**：创建模型实例，并将模型移动到之前设置的设备上。\n",
    "9. **定义损失函数和优化器**：使用交叉熵损失函数和随机梯度下降（SGD）优化器。\n",
    "10. **训练循环**：对于每个训练周期，迭代训练集中的批次数据进行训练。计算输出、损失、梯度，然后更新模型参数。\n",
    "11. **验证过程**：在每个训练周期后，使用验证集评估模型的性能。计算验证集上的损失和准确率。\n",
    "12. **测试模型**：使用测试集评估模型的性能。计算测试集上的准确率。\n",
    "13. **保存和加载模型**：保存训练好的模型权重参数到文件中，并可以加载之前保存的模型。\n",
    "\n",
    "总体来说，这份代码实现了一个基于 PyTorch 的手写数字识别模型的训练、验证和测试过程，并提供了保存和加载模型的功能。\n",
    "\n",
    "从上面的 PyTorch 代码可以看出，即使是一个简单的 MNIST 分类模型，也涉及了多个步骤，从数据处理到模型训练。尽管 PyTorch 为我们提供了强大的灵活性，但在一些场景下，代码的简洁性和扩展性可能受限。这正是 Lightning Fabric 发挥作用的地方。Fabric 作为一个新的、开源的库，使得在保持对训练循环完全控制的同时，快速且轻松地扩展模型成为可能。\n",
    "\n",
    "Fabric 在以下几个方面优于纯 PyTorch 代码：\n",
    "\n",
    "* **最小化更改需求**：Fabric 被引入到 PyTorch Lightning 中，旨在以最少的代码更改加速 PyTorch 的训练或推理代码，这使得它非常适合在现有的 PyTorch 项目中使用，以加速和扩展模型，而不需要大规模的重构。\n",
    "* **快速实施**：与 Lightning 的完整训练器相比，Fabric 实现起来更快。不需要重新构建代码，只需在 PyTorch 脚本中更改几行代码，就可以利用 Fabric 的功能。\n",
    "* **灵活性和控制**：Fabric 允许进行灵活的迭代式训练、元学习、交叉验证和其他类型的优化算法，而不需要深入了解框架内部。\n",
    "* **自动化设备和精度支持**：Fabric 自动将模型和数据放置到设备上，并支持混合和双精度，从而减小内存占用。\n",
    "\n",
    "下面我们来看看如何用 Fabric 改进这个模型。\n",
    "\n",
    "[When to Use PyTorch Lightning or Lightning Fabric](https://lightning.ai/blog/pytorch-lightning-and-fabric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "尽管 Fabric 提供了一些显著的优势，比如简化代码和扩展模型的能力，但在可读性和维护性方面可能仍然存在一些挑战。为了进一步提高代码的可读性、可维护性和模块化，最终迁移到 PyTorch Lightning 就显得尤为重要。PyTorch Lightning 提供了一个更高层次的抽象，使得研究人员可以专注于模型的设计，而不是训练过程的实现，从而在代码的组织和管理上实现质的飞跃。\n",
    "\n",
    "下面我们来看看如何用 Lightning 重构这个模型。\n",
    "\n",
    "![PyTorch2Lightning](images/pl_quick_start_full_compressed.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LightningDataModule\n",
    "\n",
    "首先，我们需要将数据的加载、预处理、划分等操作封装到 `LightningDataModule` 中，这样可以使得数据的加载、预处理、划分等操作更加模块化、可读性更强、可维护性更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T12:30:33.767889105Z",
     "start_time": "2023-06-18T12:30:33.722514536Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = os.getcwd(), batch_size: int = 64, num_workers: int = 4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download the MNIST dataset\n",
    "        datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val/test datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = datasets.MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = datasets.MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                          persistent_workers=True, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                          persistent_workers=True, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers,\n",
    "                          persistent_workers=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样一来，我们就可以将数据的加载、预处理、划分等操作从模型中分离出来，使得模型的定义更加简洁。\n",
    "\n",
    "### LightningModule\n",
    "\n",
    "接下来，我们需要将模型的定义、前向传播、损失函数、优化器等操作封装到 `LightningModule` 中，这样可以使得模型的定义、前向传播、损失函数、优化器等操作更加模块化、可读性更强、可维护性更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T12:30:36.710665306Z",
     "start_time": "2023-06-18T12:30:36.706573125Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class MNISTModule(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # extract x and y from the batch\n",
    "        x, y = batch\n",
    "\n",
    "        # forward pass\n",
    "        logits = self(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        # Logging to the built-in logger\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # extract x and y from the batch\n",
    "        x, y = batch\n",
    "\n",
    "        # forward pass\n",
    "        logits = self(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        # calculate the accuracy\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        correct = (predicted == y).sum().item()\n",
    "\n",
    "        # Logging to the built-in logger\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', correct / len(y), prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # extract x and y from the batch\n",
    "        x, y = batch\n",
    "\n",
    "        # forward pass\n",
    "        logits = self(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        # calculate the accuracy\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        correct = (predicted == y).sum().item()\n",
    "\n",
    "        # Logging to the built-in logger\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('test_acc', correct / len(y), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {'test_loss': loss, 'correct': correct, 'total': len(y)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "在定义好 `LightningDataModule` 和 `LightningModule` 后，我们就可以使用 `Trainer` 来训练模型了。\n",
    "\n",
    "`Trainer` 是 Lightning 提供的一个用于训练模型的类，它封装了训练过程中的大量细节，使得训练过程更加简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T12:32:22.025225245Z",
     "start_time": "2023-06-18T12:30:55.516774120Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rizhong/anaconda3/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:509: UserWarning: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/rizhong/Documents/r/100400_数据挖掘/lightning_logs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e8bcb7fcdb42d38fe15ed0c1e58f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.2754228703338169\n",
      "Restoring states from the checkpoint path at /home/rizhong/Documents/r/100400_数据挖掘/.lr_find_4087d7d5-a316-4e21-b697-eed2293aa1bd.ckpt\n",
      "Restored all states from the checkpoint at /home/rizhong/Documents/r/100400_数据挖掘/.lr_find_4087d7d5-a316-4e21-b697-eed2293aa1bd.ckpt\n",
      "2023-11-13 11:15:10.388366: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 11:15:12.433584: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-13 11:15:16.446176: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib:/usr/lib/x86_64-linux-gnu\n",
      "2023-11-13 11:15:16.446295: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib:/usr/lib/x86_64-linux-gnu\n",
      "2023-11-13 11:15:16.446299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | fc1  | Linear           | 100 K \n",
      "1 | fc2  | Linear           | 8.3 K \n",
      "2 | fc3  | Linear           | 650   \n",
      "3 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "109 K     Trainable params\n",
      "0         Non-trainable params\n",
      "109 K     Total params\n",
      "0.438     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1466d9d1131d4400b0e12dd34be5aedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d168e2e8a834066bf0b3d427680b7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9f0369844645f69270b924f5b4e000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c16672cbb44a699824e6f796e4ce8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5285fa5fd524d04823da7cac84108ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678129f059c84e62a38be75dc4b9b745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1650a33912f14f9caac08edec33fe6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddad34bab7f34ff6ab8cbfe47eea35a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e33046ac2f34cd2a5d3c67dfa77dfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f1d2b8732b4f549634b2938362e31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize data module and model\n",
    "data_module = MNISTDataModule()\n",
    "model = MNISTModule()\n",
    "\n",
    "callbacks = [\n",
    "    # Early stopping callback to prevent overfitting\n",
    "    pl.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n",
    "    # Model checkpoint callback to save the best model(s)\n",
    "    pl.callbacks.ModelCheckpoint(monitor='val_loss')\n",
    "]\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,  # Number of epochs, if -1, runs indefinitely\n",
    "    precision='16-mixed',  # precision of training, default is 32-bit\n",
    "    callbacks=callbacks,  # callbacks defined above\n",
    "    accelerator='cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    # device to use for training\n",
    ")\n",
    "\n",
    "# Tune the model hyperparameters\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "tuner = Tuner(trainer)\n",
    "\n",
    "# # Find the optimal batch size that maximizes GPU utilization\n",
    "# tuner.scale_batch_size(model, data_module)\n",
    "\n",
    "# Find the \"optimal\" learning rate (BE CAREFUL)\n",
    "tuner.lr_find(model, data_module)\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8afadc27761478cb3ba598e7400aac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9750999808311462     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10348556190729141    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001B[36m \u001B[0m\u001B[36m     test_acc_epoch      \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.9750999808311462    \u001B[0m\u001B[35m \u001B[0m│\n",
       "│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.10348556190729141   \u001B[0m\u001B[35m \u001B[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.10348556190729141,\n",
       "  'test_acc_epoch': 0.9750999808311462}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这种方式使用 PyTorch Lightning 框架可以大幅简化代码，提高模型的可读性和可维护性，同时保持了灵活性和强大的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MONAI 的安装\n",
    "\n",
    "MONAI 的安装非常简单。\n",
    "\n",
    "若想用 pip 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "pip install monai\n",
    "```\n",
    "\n",
    "若想用 conda 安装，可以使用以下命令：\n",
    "\n",
    "```bash\n",
    "conda install monai -c conda-forge\n",
    "```\n",
    "\n",
    "安装完成后，可以使用以下命令查看版本信息：\n",
    "\n",
    "```bash\n",
    "python -c \"import monai; print(monai.__version__)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-13 11:16:58.005398: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib:/usr/lib/x86_64-linux-gnu\n",
      "2023-11-13 11:16:58.005472: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib:/usr/lib/x86_64-linux-gnu\n",
      "2023-11-13 11:16:58.005495: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import monai\" || pip install 'monai[all]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONAI 为什么值得关注\n",
    "\n",
    "MONAI（Medical Open Network for AI）是专门为医学影像研究而设计的深度学习框架。它基于 PyTorch，提供了一系列针对医学影像处理的优化工具和方法。以下是一些让 MONAI 引人注目的特点：\n",
    "\n",
    "1. **多维医学影像数据的灵活预处理**：MONAI 提供了一系列预处理和增强工具，这些工具可以处理不同维度和不同模态（如 CT、MRI）的医学影像数据。这大大提高了数据的可用性和质量。\n",
    "2. **组合式和可移植的 API**：MONAI 设计了简洁而强大的 API，使得它可以轻松地集成到现有的医学影像处理和分析工作流程中。这种组合式的方法提供了极高的灵活性，允许研究者根据需要自定义流程。\n",
    "3. **针对特定领域的网络、损失函数和评估指标实现**：MONAI 提供了一系列专为医学影像分析定制的网络架构、损失函数和评估指标。这些实现考虑到了医学影像数据的特殊性，如不同类型的数据不平衡、高维度和高解析度等问题。\n",
    "4. **与现有的医学影像分析工具兼容**：MONAI 可以与其他流行的医学影像工具（如 ITK、SimpleITK、DICOM、NiBabel 等）无缝集成，方便从事此领域研究的科学家和工程师们导入和导出数据。\n",
    "\n",
    "<img alt=\"MONAI Architecture\" src=\"images/MONAI_arch_modules.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 医学影像数据预处理方法\n",
    "\n",
    "在 MONAI 中，数据处理方法（也称为 transform）是处理医学影像数据的核心组成部分。这些方法旨在提高医学影像数据的质量和可用性，从而提升模型训练和评估的效果。MONAI 提供了一系列基于字典的预处理方法，为处理复杂的医学影像数据提供了强大的灵活性和便利性。以下是一些常用的基于字典的预处理方法：\n",
    "\n",
    "- `LoadImaged`：加载影像数据。能够处理各种不同格式的医学影像，如 NIFTI、DICOM，以及标准的图像格式，如 PNG、JPG 等。\n",
    "- `EnsureChannelFirstd`：确保影像数据的通道维度位于第一位，符合 PyTorch 的默认通道顺序。\n",
    "- `MaskIntensityd`：根据给定的遮罩，将影像数据中的非感兴趣区域置为指定的值，以提高模型的训练效果。\n",
    "- `AsDiscreted`：将影像数据转换为离散的 One-Hot 编码，适用于分类任务。\n",
    "- `Orientationd`：调整影像数据的方向，使其符合标准方向，解决由于设备差异导致的方向不一致问题。\n",
    "- `Resized`：调整影像的大小，实现输入数据尺寸的统一。\n",
    "- `NormalizeIntensityd`：标准化影像的强度值，消除设备间的强度差异。\n",
    "- `RandRotate90d`：随机旋转影像，增强模型从不同角度识别图像的能力。\n",
    "- `RandFlipd`：随机翻转影像，增强模型对影像方向变化的鲁棒性。\n",
    "- `RandGaussianNoised`：向影像中添加随机高斯噪声，提升模型处理噪声数据的能力。\n",
    "- `RandGaussianSmoothd`：对影像进行随机高斯平滑处理，提升模型处理噪声数据的能力。\n",
    "- `RandScaleIntensityd`：对影像进行随机强度缩放，提升模型处理不同强度数据的能力。\n",
    "- `Rand2DElasticd`：对影像进行随机二维弹性变形，提升模型处理不同形变数据的能力。\n",
    "- `CropForegroundd`：剪裁掉影像中的前景部分，专注于特定结构或区域的分析。\n",
    "- `RandCropByPosNegLabeld`：根据正负标签随机剪裁影像，平衡正负样本。\n",
    "- `ToTensord`：将影像转换为 PyTorch 张量，为深度学习做准备。\n",
    "- `Lambdad`：允许自定义数据处理方法，增加数据处理的灵活性。\n",
    "- `Compose`：将多个数据处理方法组合起来，构建数据处理流程。\n",
    "\n",
    "这些预处理方法可以灵活配置和组合，以针对特定医学影像任务构建有效的数据处理流程。在 MONAI 中，这些方法通常以字典形式传递，使得每个处理步骤都可以访问到数据集中的相关信息，如影像路径、标签、元数据等，增加处理过程的透明度和可控性。\n",
    "\n",
    "#### 数据加载\n",
    "\n",
    "在 MONAI 中，数据加载是指将医学影像数据加载到内存中，以便后续的预处理和模型训练。\n",
    "\n",
    "除了常规的 `Dataset` 外，MONAI 提供了 `CacheDataset` 和 `SmartCacheDataset` 两种数据集，用于优化大规模医学影像数据的加载过程。\n",
    "\n",
    "- `CacheDataset`：预先加载并处理整个数据集，然后将其存储在内存中。这对于相对较小的数据集非常有效，因为它减少了每个 epoch 时的计算开销。\n",
    "- `SmartCacheDataset`：仅预加载和处理数据集的一部分，并在训练过程中逐渐替换旧的数据项。这对于大型数据集特别有用，因为它减少了初始加载时间，并且可以动态调整数据集的内容。\n",
    "\n",
    "#### 模型构建\n",
    "\n",
    "MONAI 提供了多种内置模型，特别是针对医学影像任务设计的模型，如 UNet、VNet、DenseNet、HighResNet 等。这些模型可以直接用于医学影像分析任务，也可以用作预训练模型的基础。\n",
    "\n",
    "#### 模型验证\n",
    "\n",
    "在处理尺寸较大的医学影像数据时，我们通常会使用前面提到的 `RandCropByPosNegLabeld` 等方法对数据进行剪裁，以减少内存占用。\n",
    "\n",
    "在模型验证阶段，MONAI 的 `SlidingWindowInferer` 可以用于处理大尺寸的医学影像。它将大影像切分成小窗口，逐个送入模型进行推理，然后将结果拼接回完整的影像。这种方法特别适用于 3D 影像或大尺寸的 2D 影像。\n",
    "\n",
    "#### 模型评估\n",
    "\n",
    "MONAI 支持多种损失函数和评估指标，适用于不同的医学影像分析任务：\n",
    "\n",
    "- 损失函数：Dice 损失、交叉熵损失、Focal 损失等，这些损失函数可以帮助处理医学影像中常见的类别不平衡问题。\n",
    "- 评估指标：Dice 系数、灵敏度、特异性等，这些指标用于量化模型在医学影像分析任务上的性能。\n",
    "\n",
    "\n",
    "\n",
    "通过结合使用这些高效的数据加载方法、强大的模型架构、灵活的推理工具以及有效的损失函数和评估指标，MONAI 能够在医学影像分析领域提供出色的性能和灵活性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现一个简单的医学影像分割模型\n",
    "\n",
    "我们将结合 MONAI 和 PyTorch Lightning 来训练一个用于分割视网膜血管的模型。我们选用的是 DRIVE（Digital Retinal Images for Vessel Extraction）数据集，这是一个广泛使用的医学影像数据集，专门用于视网膜血管分割任务。该数据集包含 20 个训练样本和 20 个测试样本，每个样本包含以下 3 个文件：\n",
    "\n",
    "1. **视网膜图像**：彩色的眼底照片，用于作为模型的输入。\n",
    "2. **血管分割图像**：标记了视网膜血管的图像，用作训练和评估模型时的真值（ground truth）。\n",
    "3. **视盘分割图像**：标记了视网膜视盘区域的图像，有时用于提高分割任务的准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理\n",
    "\n",
    "首先，我们需要对 DRIVE 数据集进行预处理。使用 MONAI 的预处理功能，我们可以轻松地加载、标准化和转换这些图像，使其适合用于深度学习模型。预处理步骤可能包括：\n",
    "\n",
    "- 转换彩色视网膜图像为灰度图像。\n",
    "- 标准化图像的尺寸和强度值。\n",
    "- 应用数据增强，如旋转、翻转、弹性变形等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:09:22.040900352Z",
     "start_time": "2023-11-13T15:09:21.989944430Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import random_split\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    MaskIntensityd,\n",
    "    EnsureChannelFirstd,\n",
    "    NormalizeIntensityd,\n",
    "    ScaleIntensityd,\n",
    "    AsDiscreted,\n",
    "    Resized,\n",
    "    RandRotate90d,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    Rand2DElasticd,\n",
    "    ToTensord,\n",
    ")\n",
    "\n",
    "\n",
    "class DRIVEDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 data_dir: str,\n",
    "                 batch_size: int = 64,\n",
    "                 num_workers: int = 4,\n",
    "                 train_val_split: float = 0.8,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_val_split = train_val_split\n",
    "\n",
    "    def get_train_transform(self):\n",
    "        return Compose([\n",
    "            LoadImaged(keys=[\"image\", \"label\", \"mask\"], reader=\"ITKReader\"),\n",
    "            EnsureChannelFirstd(keys=[\"image\", \"label\", \"mask\"]),\n",
    "            ScaleIntensityd(keys=[\"label\", \"mask\"]),\n",
    "            MaskIntensityd(keys=[\"image\"], mask_key=\"mask\", select_fn=lambda x: x > 0.5),\n",
    "            Resized(keys=[\"image\", \"label\", \"mask\"], spatial_size=(512, 512)),\n",
    "            # Rand2DElasticd(keys=[\"image\", \"label\", \"mask\"], spacing=(50, 50), magnitude_range=(5, 15)),\n",
    "            RandFlipd(keys=[\"image\", \"label\", \"mask\"], prob=0.5),\n",
    "            RandRotate90d(keys=[\"image\", \"label\", \"mask\"], prob=0.5, spatial_axes=(0, 1)),\n",
    "            RandGaussianNoised(keys=[\"image\"], prob=0.5),\n",
    "            # RandGaussianSmoothd(keys=[\"image\"], prob=0.5),\n",
    "            NormalizeIntensityd(keys=[\"image\"]),\n",
    "            AsDiscreted(keys=\"label\", to_onehot=2, threshold=0.5),\n",
    "            ToTensord(keys=[\"image\", \"label\", \"mask\"]),\n",
    "        ])\n",
    "\n",
    "    def get_val_transform(self):\n",
    "        return Compose([\n",
    "            LoadImaged(keys=[\"image\", \"label\", \"mask\"], reader=\"ITKReader\"),\n",
    "            EnsureChannelFirstd(keys=[\"image\", \"label\", \"mask\"]),\n",
    "            ScaleIntensityd(keys=[\"label\", \"mask\"]),\n",
    "            MaskIntensityd(keys=[\"image\"], mask_key=\"mask\", select_fn=lambda x: x > 0.5),\n",
    "            Resized(keys=[\"image\", \"label\", \"mask\"], spatial_size=(512, 512)),\n",
    "            NormalizeIntensityd(keys=[\"image\"]),\n",
    "            AsDiscreted(keys=\"label\", to_onehot=2, threshold=0.5),\n",
    "            ToTensord(keys=[\"image\", \"label\", \"mask\"]),\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            # Paths for training images and labels\n",
    "            train_image_dir = os.path.join(self.data_dir, 'training', 'images')\n",
    "            train_label_dir = os.path.join(self.data_dir, 'training', '1st_manual')\n",
    "            train_mask_dir = os.path.join(self.data_dir, 'training', 'mask')\n",
    "\n",
    "            # Create file lists\n",
    "            train_image_files = sorted(\n",
    "                [os.path.join(train_image_dir, f) for f in os.listdir(train_image_dir) if f.endswith('.tif')])\n",
    "            train_label_files = sorted(\n",
    "                [os.path.join(train_label_dir, f) for f in os.listdir(train_label_dir) if f.endswith('.gif')])\n",
    "            train_mask_files = sorted(\n",
    "                [os.path.join(train_mask_dir, f) for f in os.listdir(train_mask_dir) if f.endswith('.gif')])\n",
    "\n",
    "            # Pairing images with labels\n",
    "            train_files = [{\"image\": img, \"label\": lbl, \"mask\": msk} for img, lbl, msk in\n",
    "                           zip(train_image_files, train_label_files, train_mask_files)]\n",
    "\n",
    "            # Splitting dataset into training and validation sets\n",
    "            train_len = int(len(train_files) * self.train_val_split)\n",
    "            val_len = len(train_files) - train_len\n",
    "            self.train_dataset, self.val_dataset = random_split(train_files, [train_len, val_len])\n",
    "\n",
    "            # Applying transforms\n",
    "            self.train_dataset = CacheDataset(self.train_dataset, transform=self.get_train_transform())\n",
    "            self.val_dataset = CacheDataset(self.val_dataset, transform=self.get_val_transform())\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            # Paths for test images\n",
    "            test_image_dir = os.path.join(self.data_dir, 'test', 'images')\n",
    "            test_label_dir = os.path.join(self.data_dir, 'test', '1st_manual')\n",
    "            test_mask_dir = os.path.join(self.data_dir, 'test', 'mask')\n",
    "\n",
    "            # Create file list\n",
    "            test_image_files = sorted(\n",
    "                [os.path.join(test_image_dir, f) for f in os.listdir(test_image_dir) if f.endswith('.tif')])\n",
    "            test_label_files = sorted(\n",
    "                [os.path.join(test_label_dir, f) for f in os.listdir(test_label_dir) if f.endswith('.gif')])\n",
    "            test_mask_files = sorted(\n",
    "                [os.path.join(test_mask_dir, f) for f in os.listdir(test_mask_dir) if f.endswith('.gif')])\n",
    "\n",
    "            # Preparing test dataset\n",
    "            test_files = [{\"image\": img, \"label\": lbl, \"mask\": msk} for img, lbl, msk in\n",
    "                          zip(test_image_files, test_label_files, test_mask_files)]\n",
    "            self.test_dataset = CacheDataset(test_files, transform=self.get_val_transform())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 模型定义\n",
    "\n",
    "接下来，我们需要定义一个用于分割视网膜血管的模型。我们可以使用 `UNet` 模型，它是一个经典的用于医学影像分割的模型。\n",
    "\n",
    "UNet 的核心设计理念是一个对称的 “U” 形结构，包含一个收缩（下采样）路径和一个扩展（上采样）路径。收缩路径通过一系列卷积和池化操作捕获图像的上下文信息，而扩展路径则通过上采样和卷积操作恢复图像的空间维度，使模型能够进行精确的定位。\n",
    "\n",
    "在 MONAI 框架中，UNet 已经被实现，并且提供了易于使用的接口。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet\n",
    "from monai.networks import one_hot\n",
    "import lightning.pytorch as pl\n",
    "from monai.losses import DiceLoss, FocalLoss, DiceFocalLoss\n",
    "from monai.metrics import DiceMetric, MeanIoU\n",
    "import torchvision\n",
    "\n",
    "class SegmentationModule(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 learning_rate=0.01,\n",
    "                 backbone='unet',\n",
    "                 loss='dice',\n",
    "                 optimizer='Adam',\n",
    "                 accuracy='dice',\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if backbone.lower() == 'unet':\n",
    "            self.model = UNet(\n",
    "                spatial_dims=2,\n",
    "                in_channels=3,\n",
    "                out_channels=2,\n",
    "                channels=(16, 32, 64, 128, 256),\n",
    "                strides=(2, 2, 2, 2),\n",
    "                num_res_units=2,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f'Backbone {backbone} not implemented. '\n",
    "                                      f'Please choose from [unet].')    \n",
    "        \n",
    "        if loss.lower() == 'dice':\n",
    "            self.loss = DiceLoss( softmax=True)\n",
    "        elif loss.lower() == 'focal':\n",
    "            self.loss = FocalLoss()\n",
    "        elif loss.lower() == 'dicefocal':\n",
    "            self.loss = DiceFocalLoss( softmax=True)\n",
    "        else:\n",
    "            raise NotImplementedError(f'Loss {loss} not implemented. '\n",
    "                                      f'Please choose from [dice, focal, dicefocal].')\n",
    "        \n",
    "        if optimizer.lower() == 'adam':\n",
    "            self.optimizer = torch.optim.Adam\n",
    "        elif optimizer.lower() == 'sgd':\n",
    "            self.optimizer = torch.optim.SGD\n",
    "        else:\n",
    "            raise NotImplementedError(f'Optimizer {optimizer} not implemented. '\n",
    "                                      f'Please choose from [Adam, SGD].')\n",
    "        \n",
    "        if accuracy.lower() == 'dice':\n",
    "            self.accuracy = DiceMetric(include_background=False, reduction='mean')\n",
    "        elif accuracy.lower() == 'iou':\n",
    "            self.accuracy = MeanIoU(include_background=False, reduction='mean')\n",
    "        else:\n",
    "            raise NotImplementedError(f'Accuracy {accuracy} not implemented. '\n",
    "                                      f'Please choose from [dice, iou].')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def prepare_batch(self, batch):\n",
    "        return batch['image'], batch['label']\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = self.prepare_batch(batch)\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('train_loss', loss.mean(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = self.prepare_batch(batch)\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        y_hat_onehot = one_hot(torch.argmax(y_hat, dim=1, keepdim=True), num_classes=2)\n",
    "        acc = self.accuracy(y_hat_onehot, y)\n",
    "        self.log('val_loss', loss.mean(), prog_bar=True)\n",
    "        self.log('val_acc', acc.mean(), prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = self.prepare_batch(batch)\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        y_hat_onehot = one_hot(torch.argmax(y_hat, dim=1, keepdim=True), num_classes=2)\n",
    "        acc = self.accuracy(y_hat_onehot, y)\n",
    "        self.log('test_loss', loss.mean(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('test_acc', acc.mean(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'test_loss': loss.mean(), 'test_acc': acc.mean()}\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T15:09:24.325559689Z",
     "start_time": "2023-11-13T15:09:24.316265486Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 训练模型\n",
    "\n",
    "现在，我们可以使用 PyTorch Lightning 来训练模型了。我们将使用 `Trainer` 类来训练模型，它封装了训练过程中的大量细节，使得训练过程更加简单。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "Loading dataset:   0%|          | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Loading dataset:  25%|██▌       | 4/16 [00:00<00:00, 39.66it/s]\u001B[A\n",
      "Loading dataset:  50%|█████     | 8/16 [00:00<00:00, 33.27it/s]\u001B[A\n",
      "Loading dataset: 100%|██████████| 16/16 [00:00<00:00, 35.96it/s]\u001B[A\n",
      "\n",
      "Loading dataset:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Loading dataset: 100%|██████████| 4/4 [00:00<00:00, 31.39it/s]\u001B[A\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | UNet     | 1.6 M \n",
      "1 | loss  | DiceLoss | 0     \n",
      "-----------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.503     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2efc06ad9dc9436289b7cca2b642781c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c698662ab3048b2b9b9bf406b6cdf5e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba4b97bdf7e54f89a8f3251f1a129846"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49288693384a40998a6f74ab5623e9b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64aeaa502d114327abfe282e34cba415"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ef6cda2fe314577890d4fefab3c5f8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0326bd2f229e41818b4da82baee967ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "286145963b0d479a91633028c879697e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "186c6c430dfa44e2b6d373f6651472a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de65d56e799e4b2097ee70cb136c450b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46aa5523f74c419abdc9001a8a37b255"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51299cc3441c4b61b02f3e3e7e617c87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79c6a5b69f1243eb8852eedfd834af85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c3a1a3a99274e4795cb72e49db56ac6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd226a33cce341beb545c87e18ad9b03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7b7301e20774bae9bb729fa155b695c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82358c67a77d4b2b977dc302bc921583"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6abd46d4f9244201a7d50c97caf4a6d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "434cb8cbee814a27b9b7217cd0088373"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bca00bac0964411cbed009bf31c8da41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae5c7bd05667436babca1f6794273f5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdc44a7787c04944b1445cbb3812780a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\n",
      "Loading dataset:   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "Loading dataset:  15%|█▌        | 3/20 [00:00<00:00, 21.92it/s]\u001B[A\n",
      "Loading dataset:  30%|███       | 6/20 [00:00<00:00, 24.10it/s]\u001B[A\n",
      "Loading dataset:  45%|████▌     | 9/20 [00:00<00:00, 24.80it/s]\u001B[A\n",
      "Loading dataset:  60%|██████    | 12/20 [00:00<00:00, 23.95it/s]\u001B[A\n",
      "Loading dataset:  75%|███████▌  | 15/20 [00:00<00:00, 23.33it/s]\u001B[A\n",
      "Loading dataset: 100%|██████████| 20/20 [00:00<00:00, 24.28it/s]\u001B[A\n",
      "Restoring states from the checkpoint path at lightning_logs/version_6/checkpoints/epoch=19-step=80.ckpt\n",
      "Loaded model weights from the checkpoint at lightning_logs/version_6/checkpoints/epoch=19-step=80.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d07afbdb0b9e4b0496aadafeed3c7759"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_acc_epoch      \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.7356123924255371    \u001B[0m\u001B[35m \u001B[0m│\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.15223458409309387   \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7356123924255371     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.15223458409309387    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'test_loss_epoch': 0.15223458409309387,\n  'test_acc_epoch': 0.7356123924255371}]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning.pytorch import seed_everything\n",
    "import torch\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Initialize data module and model\n",
    "data_module = DRIVEDataModule(data_dir='data/DRIVE', batch_size=4, num_workers=4)\n",
    "model = SegmentationModule(learning_rate=0.01)\n",
    "\n",
    "callbacks = [\n",
    "    # Early stopping callback to prevent overfitting\n",
    "    EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n",
    "    # Model checkpoint callback to save the best model(s)\n",
    "    ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=5, filename='{epoch}-{val_loss:.3f}-{val_acc:.4f}')\n",
    "]\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,  # Number of epochs, if -1, runs indefinitely\n",
    "    # precision='bf16',  # precision of training, default is 32-bit\n",
    "    callbacks=callbacks,  # callbacks defined above\n",
    "    accelerator='cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    # device to use for training\n",
    ")\n",
    "\n",
    "# Tune the model hyperparameters\n",
    "tuner = Tuner(trainer)\n",
    "\n",
    "# # Find the optimal batch size that maximizes GPU utilization\n",
    "# tuner.scale_batch_size(model, data_module)\n",
    "\n",
    "# # Find the \"optimal\" learning rate (BE CAREFUL)\n",
    "# tuner.lr_find(model, data_module)\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model=model, datamodule=data_module)\n",
    "\n",
    "# Test the model\n",
    "trainer.test(model=model, datamodule=data_module, ckpt_path=\"lightning_logs/version_6/checkpoints/epoch=19-step=80.ckpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T15:10:52.727922977Z",
     "start_time": "2023-11-13T15:09:31.150458637Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
